{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\Python\\Titanic\\data\\test.csv\n",
      "C:\\Projects\\Python\\Titanic\\data\\train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # wrapper to use function from sklearn\n",
    "\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV #training and testing data split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('C:\\Projects\\Python\\Titanic\\data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "# import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "#import sns for better plots, it is handy to manage subplots\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Projects/Python/titanic/data/train.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Survived  Pclass  Sex   Age  SibSp  Parch     Fare  title_Master  \\\n",
      "0           0       3    1  22.0      1      0   7.2500             0   \n",
      "1           1       1    0  38.0      1      0  71.2833             0   \n",
      "2           1       3    0  26.0      0      0   7.9250             0   \n",
      "3           1       1    0  35.0      1      0  53.1000             0   \n",
      "4           0       3    1  35.0      0      0   8.0500             0   \n",
      "..        ...     ...  ...   ...    ...    ...      ...           ...   \n",
      "886         0       2    1  27.0      0      0  13.0000             0   \n",
      "887         1       1    0  19.0      0      0  30.0000             0   \n",
      "888         0       3    0  22.0      1      2  23.4500             0   \n",
      "889         1       1    1  26.0      0      0  30.0000             0   \n",
      "890         0       3    1  32.0      0      0   7.7500             0   \n",
      "\n",
      "     title_Miss  title_Mr  title_Mrs  title_Other  embarked_C  embarked_Q  \\\n",
      "0             0         1          0            0           0           0   \n",
      "1             0         0          1            0           1           0   \n",
      "2             1         0          0            0           0           0   \n",
      "3             0         0          1            0           0           0   \n",
      "4             0         1          0            0           0           0   \n",
      "..          ...       ...        ...          ...         ...         ...   \n",
      "886           0         0          0            1           0           0   \n",
      "887           1         0          0            0           0           0   \n",
      "888           1         0          0            0           0           0   \n",
      "889           0         1          0            0           1           0   \n",
      "890           0         1          0            0           0           1   \n",
      "\n",
      "     embarked_S  \n",
      "0             1  \n",
      "1             0  \n",
      "2             1  \n",
      "3             1  \n",
      "4             1  \n",
      "..          ...  \n",
      "886           1  \n",
      "887           1  \n",
      "888           1  \n",
      "889           0  \n",
      "890           0  \n",
      "\n",
      "[891 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# get mean aged based on titel\n",
    "data['Initial']=0\n",
    "for i in data:\n",
    "    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n",
    "\n",
    "data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n",
    "                    ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\n",
    "\n",
    "## Assigning the NaN Values with the Ceil values of the mean ages\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Mr'),'Age']=33\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Mrs'),'Age']=36\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Master'),'Age']=5\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Miss'),'Age']=22\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Other'),'Age']=46\n",
    "data.Age.isnull().any() #So no null values left finally \n",
    "\n",
    "# drop some irrelevant information\n",
    "droplist = ['PassengerId', 'Name', 'Cabin', 'Ticket']\n",
    "data.drop(droplist,axis=1,inplace=True)\n",
    "\n",
    "# one hot encoding of categorical values\n",
    "ohe_initial = pd.get_dummies(data['Initial'], prefix='title')\n",
    "ohe_embarked = pd.get_dummies(data['Embarked'], prefix='embarked')\n",
    "# turn sex into integers instead of string\n",
    "data['Sex'] = data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "new_df = pd.merge(data, ohe_initial, left_index=True, right_index=True)\n",
    "final_df = pd.merge(new_df, ohe_embarked, left_index=True, right_index=True)\n",
    "droplist = ['Initial', 'Embarked']\n",
    "final_df.drop(droplist,axis=1,inplace=True)\n",
    "\n",
    "print(final_df)\n",
    "\n",
    "#================================= end data preprocessing ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape before: (757, 14)\n",
      "xtrain shape after: (757, 14)\n",
      "(134, 14)\n",
      "(757, 14)\n",
      "(757, 1)\n",
      "(134, 14)\n",
      "(134, 1)\n",
      "[[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "#========================= start data prep\n",
    "train, test= train_test_split(final_df, test_size=0.15, stratify=final_df[\"Survived\"])\n",
    "\n",
    "Xtrain = train[train.columns[1:]]\n",
    "ytrain = train[train.columns[:1]]\n",
    "Xtest = test[test.columns[1:]]\n",
    "ytest = test[test.columns[:1]]\n",
    "\n",
    "# for cross validation we need to take the complete dataset and pass it as it takes care of test train split\n",
    "X = final_df[final_df.columns[1:]]\n",
    "y = final_df[final_df.columns[:1]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_normed = scaler.fit_transform(X)\n",
    "\n",
    "xtrain_val = Xtrain.values\n",
    "print(\"xtrain shape before: \" + str(xtrain_val.shape))\n",
    "#xtrain_val = np.reshape(xtrain_val, (-1, 757))\n",
    "print(\"xtrain shape after: \" + str(xtrain_val.shape))\n",
    "ytrain_val = ytrain.values\n",
    "#ytrain_val = np.reshape(ytrain_val, (-1, 757))\n",
    "\n",
    "xtest_val = Xtest.values\n",
    "print(xtest_val.shape)\n",
    "#xtest_val = np.reshape(xtest_val, (-1, 134))\n",
    "ytest_val = ytest.values\n",
    "#ytest_val = np.reshape(ytest_val, (-1, 134))\n",
    "\n",
    "print(xtrain_val.shape)\n",
    "print(ytrain_val.shape)\n",
    "print(xtest_val.shape)\n",
    "print(ytest_val.shape)\n",
    "\n",
    "xtrain_normed = scaler.fit_transform(xtrain_val)\n",
    "xtest_normed = scaler.fit_transform(xtest_val)\n",
    "print(ytest_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 14)\n",
      "(134, 1)\n",
      "Best: 0.834969 using {'batch_size': 90, 'epochs': 5, 'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "def create_model(optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(196, activation='relu'))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    #model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    #model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(xtest_normed.shape)\n",
    "print(ytest_val.shape)\n",
    "\n",
    "#model.fit(xtrain_normed, ytrain_val, validation_data=(xtest_normed, ytest_val), epochs=50, batch_size=4, verbose=2)\n",
    "sk_model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "#scores = cross_val_score(sk_model, X_normed, y, cv=k_fold, scoring='accuracy')\n",
    "k_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "#batch_size = [4, 8, 16, 24, 32, 64]\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "batch_size = [64, 74, 90, 128]\n",
    "epochs = [3, 5, 8, 10, 12]\n",
    "parameter_grid = dict(optimizer=optimizer, batch_size=batch_size, epochs=epochs)\n",
    "#Best: 0.834969 using {'batch_size': 90, 'epochs': 5, 'optimizer': 'Nadam'}\n",
    "\n",
    "gs = GridSearchCV(sk_model, parameter_grid, cv=k_fold, scoring='accuracy', n_jobs=6)\n",
    "grid_result = gs.fit(X_normed, y)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 0s 554us/step - loss: 0.6512 - accuracy: 0.7143\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 0s 554us/step - loss: 0.5208 - accuracy: 0.8043\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 0s 554us/step - loss: 0.4538 - accuracy: 0.8257\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 0s 665us/step - loss: 0.4197 - accuracy: 0.8346\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 0s 554us/step - loss: 0.3803 - accuracy: 0.8492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a414b5de50>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refit the model using the optimal parameters\n",
    "opti_model = create_model('Nadam')\n",
    "opti_model.fit(X_normed, y, batch_size=90, epochs=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 76.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenneth\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\kenneth\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "xgb_dmat = xgb.DMatrix(data=xtrain_normed,label=ytrain_val)\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(xtrain_normed, ytrain_val)\n",
    "preds = xgb_model.predict(xtest_normed)\n",
    "preds = [round(value) for value in preds]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(ytest_val, preds)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Pclass                                          Name     Sex  \\\n",
      "0          892       3                              Kelly, Mr. James    male   \n",
      "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
      "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
      "3          895       3                              Wirz, Mr. Albert    male   \n",
      "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
      "\n",
      "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
      "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
      "1  47.0      1      0   363272   7.0000   NaN        S  \n",
      "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
      "3  27.0      0      0   315154   8.6625   NaN        S  \n",
      "4  22.0      1      1  3101298  12.2875   NaN        S  \n",
      "Empty DataFrame\n",
      "Columns: [PassengerId, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked, Initial]\n",
      "Index: []\n",
      "     Pclass  Sex   Age  SibSp  Parch      Fare  title_Master  title_Miss  \\\n",
      "0         3    1  34.5      0      0    7.8292             0           0   \n",
      "1         3    0  47.0      1      0    7.0000             0           0   \n",
      "2         2    1  62.0      0      0    9.6875             0           0   \n",
      "3         3    1  27.0      0      0    8.6625             0           0   \n",
      "4         3    0  22.0      1      1   12.2875             0           0   \n",
      "..      ...  ...   ...    ...    ...       ...           ...         ...   \n",
      "413       3    1  32.1      0      0    8.0500             0           0   \n",
      "414       1    0  39.0      0      0  108.9000             0           0   \n",
      "415       3    1  38.5      0      0    7.2500             0           0   \n",
      "416       3    1  32.1      0      0    8.0500             0           0   \n",
      "417       3    1   7.4      1      1   22.3583             1           0   \n",
      "\n",
      "     title_Mr  title_Mrs  title_Other  embarked_C  embarked_Q  embarked_S  \n",
      "0           1          0            0           0           1           0  \n",
      "1           0          1            0           0           0           1  \n",
      "2           1          0            0           0           1           0  \n",
      "3           1          0            0           0           0           1  \n",
      "4           0          1            0           0           0           1  \n",
      "..        ...        ...          ...         ...         ...         ...  \n",
      "413         1          0            0           0           0           1  \n",
      "414         0          1            0           1           0           0  \n",
      "415         1          0            0           0           0           1  \n",
      "416         1          0            0           0           0           1  \n",
      "417         0          0            0           1           0           0  \n",
      "\n",
      "[418 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# ==================== apply model\n",
    "test_data = pd.read_csv(\"C:/Projects/Python/titanic/data/test.csv\")\n",
    "\n",
    "print(test_data.head())\n",
    "\n",
    "# get mean aged based on titel\n",
    "test_data['Initial']=0\n",
    "for i in data:\n",
    "    test_data['Initial']=test_data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n",
    "\n",
    "test_data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Dona','Jonkheer','Col','Rev','Capt','Sir','Don'],\n",
    "                    ['Mrs','Mrs','Miss','Mr','Mr','Mrs','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\n",
    "\n",
    "\n",
    "#print(test_data.loc[(test_data.Initial=='Other'),'Age'].mean())\n",
    "\n",
    "## Assigning the NaN Values with the Ceil values of the mean ages\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Mr'),'Age']=32.1\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Mrs'),'Age']=38.9\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Master'),'Age']=7.4\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Miss'),'Age']=21.8\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Other'),'Age']=42\n",
    "print(test_data.loc[test_data.Age.isnull()]) #So no null values left finally \n",
    "\n",
    "# drop some irrelevant information\n",
    "droplist = ['PassengerId', 'Name', 'Cabin', 'Ticket']\n",
    "test_data.drop(droplist,axis=1,inplace=True)\n",
    "\n",
    "# one hot encoding of categorical values\n",
    "ohe_initial = pd.get_dummies(test_data['Initial'], prefix='title')\n",
    "ohe_embarked = pd.get_dummies(test_data['Embarked'], prefix='embarked')\n",
    "# turn sex into integers instead of string\n",
    "test_data['Sex'] = test_data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "new_df = pd.merge(test_data, ohe_initial, left_index=True, right_index=True)\n",
    "final_test_df = pd.merge(new_df, ohe_embarked, left_index=True, right_index=True)\n",
    "droplist = ['Initial', 'Embarked']\n",
    "final_test_df.drop(droplist,axis=1,inplace=True)\n",
    "print(final_test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pclass  Sex   Age  SibSp  Parch     Fare  title_Master  title_Miss  \\\n",
      "0       3    1  34.5      0      0   7.8292             0           0   \n",
      "1       3    0  47.0      1      0   7.0000             0           0   \n",
      "2       2    1  62.0      0      0   9.6875             0           0   \n",
      "3       3    1  27.0      0      0   8.6625             0           0   \n",
      "4       3    0  22.0      1      1  12.2875             0           0   \n",
      "\n",
      "   title_Mr  title_Mrs  title_Other  embarked_C  embarked_Q  embarked_S  \n",
      "0         1          0            0           0           1           0  \n",
      "1         0          1            0           0           0           1  \n",
      "2         1          0            0           0           1           0  \n",
      "3         1          0            0           0           0           1  \n",
      "4         0          1            0           0           0           1  \n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [nan]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "# data prep\n",
    "Xtest = final_test_df\n",
    "\n",
    "print(Xtest.head())\n",
    "#print(ytest.head())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_normed = scaler.fit_transform(Xtest)\n",
    "\n",
    "ypred = opti_model.predict(X_normed)\n",
    "print(ypred.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
