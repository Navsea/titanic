{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\Python\\Titanic\\data\\test.csv\n",
      "C:\\Projects\\Python\\Titanic\\data\\train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam \n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # wrapper to use function from sklearn\n",
    "from tensorflow.keras import backend as be\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV #training and testing data split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('C:\\Projects\\Python\\Titanic\\data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "# import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "#import sns for better plots, it is handy to manage subplots\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:/Projects/Python/titanic/data/train.csv\")\n",
    "data.head(5)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.15)\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Survived  Pclass  Sex   Age  SibSp  Parch     Fare Embarked Initial  \\\n",
      "0           0       3    1  22.0      1      0   7.2500        S      Mr   \n",
      "1           1       1    0  38.0      1      0  71.2833        C     Mrs   \n",
      "2           1       3    0  26.0      0      0   7.9250        S    Miss   \n",
      "3           1       1    0  35.0      1      0  53.1000        S     Mrs   \n",
      "4           0       3    1  35.0      0      0   8.0500        S      Mr   \n",
      "..        ...     ...  ...   ...    ...    ...      ...      ...     ...   \n",
      "886         0       2    1  27.0      0      0  13.0000        S   Other   \n",
      "887         1       1    0  19.0      0      0  30.0000        S    Miss   \n",
      "888         0       3    0  22.0      1      2  23.4500        S    Miss   \n",
      "889         1       1    1  26.0      0      0  30.0000        C      Mr   \n",
      "890         0       3    1  32.0      0      0   7.7500        Q      Mr   \n",
      "\n",
      "     title_Master  title_Miss  title_Mr  title_Mrs  title_Other  \n",
      "0               0           0         1          0            0  \n",
      "1               0           0         0          1            0  \n",
      "2               0           1         0          0            0  \n",
      "3               0           0         0          1            0  \n",
      "4               0           0         1          0            0  \n",
      "..            ...         ...       ...        ...          ...  \n",
      "886             0           0         0          0            1  \n",
      "887             0           1         0          0            0  \n",
      "888             0           1         0          0            0  \n",
      "889             0           0         1          0            0  \n",
      "890             0           0         1          0            0  \n",
      "\n",
      "[891 rows x 14 columns]\n",
      "     Survived  Pclass  Sex   Age  SibSp  Parch     Fare Embarked Initial  \\\n",
      "0           0       3    1  22.0      1      0   7.2500        S      Mr   \n",
      "1           1       1    0  38.0      1      0  71.2833        C     Mrs   \n",
      "2           1       3    0  26.0      0      0   7.9250        S    Miss   \n",
      "3           1       1    0  35.0      1      0  53.1000        S     Mrs   \n",
      "4           0       3    1  35.0      0      0   8.0500        S      Mr   \n",
      "..        ...     ...  ...   ...    ...    ...      ...      ...     ...   \n",
      "886         0       2    1  27.0      0      0  13.0000        S   Other   \n",
      "887         1       1    0  19.0      0      0  30.0000        S    Miss   \n",
      "888         0       3    0  22.0      1      2  23.4500        S    Miss   \n",
      "889         1       1    1  26.0      0      0  30.0000        C      Mr   \n",
      "890         0       3    1  32.0      0      0   7.7500        Q      Mr   \n",
      "\n",
      "     title_Master  title_Miss  title_Mr  title_Mrs  title_Other  embarked_C  \\\n",
      "0               0           0         1          0            0           0   \n",
      "1               0           0         0          1            0           1   \n",
      "2               0           1         0          0            0           0   \n",
      "3               0           0         0          1            0           0   \n",
      "4               0           0         1          0            0           0   \n",
      "..            ...         ...       ...        ...          ...         ...   \n",
      "886             0           0         0          0            1           0   \n",
      "887             0           1         0          0            0           0   \n",
      "888             0           1         0          0            0           0   \n",
      "889             0           0         1          0            0           1   \n",
      "890             0           0         1          0            0           0   \n",
      "\n",
      "     embarked_Q  embarked_S  \n",
      "0             0           1  \n",
      "1             0           0  \n",
      "2             0           1  \n",
      "3             0           1  \n",
      "4             0           1  \n",
      "..          ...         ...  \n",
      "886           0           1  \n",
      "887           0           1  \n",
      "888           0           1  \n",
      "889           0           0  \n",
      "890           1           0  \n",
      "\n",
      "[891 rows x 17 columns]\n",
      "     Survived  Pclass  Sex   Age  SibSp  Parch     Fare  title_Master  \\\n",
      "0           0       3    1  22.0      1      0   7.2500             0   \n",
      "1           1       1    0  38.0      1      0  71.2833             0   \n",
      "2           1       3    0  26.0      0      0   7.9250             0   \n",
      "3           1       1    0  35.0      1      0  53.1000             0   \n",
      "4           0       3    1  35.0      0      0   8.0500             0   \n",
      "..        ...     ...  ...   ...    ...    ...      ...           ...   \n",
      "886         0       2    1  27.0      0      0  13.0000             0   \n",
      "887         1       1    0  19.0      0      0  30.0000             0   \n",
      "888         0       3    0  22.0      1      2  23.4500             0   \n",
      "889         1       1    1  26.0      0      0  30.0000             0   \n",
      "890         0       3    1  32.0      0      0   7.7500             0   \n",
      "\n",
      "     title_Miss  title_Mr  title_Mrs  title_Other  embarked_C  embarked_Q  \\\n",
      "0             0         1          0            0           0           0   \n",
      "1             0         0          1            0           1           0   \n",
      "2             1         0          0            0           0           0   \n",
      "3             0         0          1            0           0           0   \n",
      "4             0         1          0            0           0           0   \n",
      "..          ...       ...        ...          ...         ...         ...   \n",
      "886           0         0          0            1           0           0   \n",
      "887           1         0          0            0           0           0   \n",
      "888           1         0          0            0           0           0   \n",
      "889           0         1          0            0           1           0   \n",
      "890           0         1          0            0           0           1   \n",
      "\n",
      "     embarked_S  \n",
      "0             1  \n",
      "1             0  \n",
      "2             1  \n",
      "3             1  \n",
      "4             1  \n",
      "..          ...  \n",
      "886           1  \n",
      "887           1  \n",
      "888           1  \n",
      "889           0  \n",
      "890           0  \n",
      "\n",
      "[891 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# get mean aged based on titel\n",
    "data['Initial']=0\n",
    "for i in data:\n",
    "    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n",
    "\n",
    "data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n",
    "                    ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\n",
    "\n",
    "## Assigning the NaN Values with the Ceil values of the mean ages\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Mr'),'Age']=33\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Mrs'),'Age']=36\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Master'),'Age']=5\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Miss'),'Age']=22\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Other'),'Age']=46\n",
    "data.Age.isnull().any() #So no null values left finally \n",
    "\n",
    "# drop some irrelevant information\n",
    "droplist = ['PassengerId', 'Name', 'Cabin', 'Ticket']\n",
    "data.drop(droplist,axis=1,inplace=True)\n",
    "\n",
    "# one hot encoding of categorical values\n",
    "ohe_initial = pd.get_dummies(data['Initial'], prefix='title')\n",
    "ohe_embarked = pd.get_dummies(data['Embarked'], prefix='embarked')\n",
    "# turn sex into integers instead of string\n",
    "data['Sex'] = data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "final_df = pd.merge(data, ohe_initial, left_index=True, right_index=True)\n",
    "print(final_df)\n",
    "final_df = pd.merge(final_df, ohe_embarked, left_index=True, right_index=True)\n",
    "print(final_df)\n",
    "droplist = ['Initial', 'Embarked']\n",
    "final_df.drop(droplist,axis=1,inplace=True)\n",
    "\n",
    "print(final_df)\n",
    "\n",
    "#================================= end data preprocessing ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.82737724  0.73769513 -0.59077149  0.43279337 -0.47367361 -0.50244517\n",
      " -0.21680296 -0.51364364  0.82723033 -0.40771358 -0.10101525 -0.48204268\n",
      " -0.30756234  0.61930636]\n",
      "xtrain shape before: (757, 14)\n",
      "xtrain shape after: (757, 14)\n",
      "(134, 14)\n",
      "(757, 14)\n",
      "(757, 1)\n",
      "(134, 14)\n",
      "(134, 1)\n"
     ]
    }
   ],
   "source": [
    "#========================= start data prep\n",
    "train, test= train_test_split(final_df, test_size=0.15, stratify=final_df[\"Survived\"])\n",
    "\n",
    "Xtrain = train[train.columns[1:]]\n",
    "ytrain = train[train.columns[:1]]\n",
    "Xtest = test[test.columns[1:]]\n",
    "ytest = test[test.columns[:1]]\n",
    "\n",
    "# for cross validation we need to take the complete dataset and pass it as it takes care of test train split\n",
    "X = final_df[final_df.columns[1:]]\n",
    "y = final_df[final_df.columns[:1]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_normed = scaler.fit_transform(X)\n",
    "\n",
    "print(X_normed[0])\n",
    "\n",
    "xtrain_val = Xtrain.values\n",
    "print(\"xtrain shape before: \" + str(xtrain_val.shape))\n",
    "print(\"xtrain shape after: \" + str(xtrain_val.shape))\n",
    "ytrain_val = ytrain.values\n",
    "\n",
    "xtest_val = Xtest.values\n",
    "print(xtest_val.shape)\n",
    "ytest_val = ytest.values\n",
    "\n",
    "print(xtrain_val.shape)\n",
    "print(ytrain_val.shape)\n",
    "print(xtest_val.shape)\n",
    "print(ytest_val.shape)\n",
    "\n",
    "xtrain_normed = scaler.fit_transform(xtrain_val)\n",
    "xtest_normed = scaler.fit_transform(xtest_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 14)\n",
      "(134, 1)\n",
      "Best: 0.845106 using {'batch_size': 92, 'epochs': 5, 'first_layer_nodes': 80, 'learning_rate': 0.05, 'optimizer': 'Adam', 'second_layer_nodes': 16}\n"
     ]
    }
   ],
   "source": [
    "def create_model(optimizer, learning_rate, first_layer_nodes, second_layer_nodes):\n",
    "    if optimizer=='SGD':\n",
    "        optimizer = SGD(learning_rate = learning_rate)\n",
    "    if optimizer=='RMSprop':\n",
    "        optimizer = RMSprop(learning_rate = learning_rate)\n",
    "    if optimizer=='Adagrad':\n",
    "        optimizer = Adagrad(learning_rate = learning_rate)\n",
    "    if optimizer=='Adadelta':\n",
    "        optimizer = Adadelta(learning_rate = learning_rate)\n",
    "    if optimizer=='Adam':\n",
    "        optimizer = Adam(learning_rate = learning_rate)\n",
    "    if optimizer=='Adamax':\n",
    "        optimizer = Adamax(learning_rate = learning_rate)\n",
    "    if optimizer=='Nadam':\n",
    "        optimizer = Nadam(learning_rate = learning_rate)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(first_layer_nodes, activation='relu'))\n",
    "    model.add(layers.Dense(second_layer_nodes, activation='relu'))\n",
    "    #model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    #model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(xtest_normed.shape)\n",
    "print(ytest_val.shape)\n",
    "\n",
    "#model.fit(xtrain_normed, ytrain_val, validation_data=(xtest_normed, ytest_val), epochs=50, batch_size=4, verbose=2)\n",
    "sk_model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "#scores = cross_val_score(sk_model, X_normed, y, cv=k_fold, scoring='accuracy')\n",
    "k_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "optimizer = [ 'RMSprop', 'Adagrad', 'Adadelta','Adam', 'Adamax']\n",
    "learning_rate = [0.03, 0.04, 0.05, 0.06]# [0.02, 0.03, 0.04, 0.05, 0.06]\n",
    "batch_size = [82, 88, 92]\n",
    "epochs = [3, 5, 6]\n",
    "first_layer_nodes = [75, 80, 85]\n",
    "second_layer_nodes = [12, 16, 20]\n",
    "parameter_grid = dict(learning_rate=learning_rate, optimizer=optimizer, batch_size=batch_size, epochs=epochs, first_layer_nodes=first_layer_nodes, second_layer_nodes=second_layer_nodes)\n",
    "#Best: 0.839476 using {'batch_size': 64, 'epochs': 3, 'learning_rate': 0.05, 'optimizer': 'Adagrad'}\n",
    "#0.840587 using {'batch_size': 74, 'epochs': 5, 'learning_rate': 0.03, 'optimizer': 'Adam'}\n",
    "#Best: 0.840612 using {'batch_size': 64, 'epochs': 3, 'learning_rate': 0.02, 'optimizer': 'Adamax'}\n",
    "#0.838352 using {'batch_size': 74, 'epochs': 3, 'first_layer_nodes': 55, 'learning_rate': 0.04, 'optimizer': 'Adagrad'}\n",
    "#Best: 0.840624 using {'batch_size': 88, 'epochs': 6, 'first_layer_nodes': 75, 'learning_rate': 0.05, 'optimizer': 'Adam'}\n",
    "#Best: 0.845106 using {'batch_size': 92, 'epochs': 5, 'first_layer_nodes': 80, 'learning_rate': 0.05, 'optimizer': 'Adam', 'second_layer_nodes': 16}\n",
    "gs = GridSearchCV(sk_model, parameter_grid, cv=k_fold, scoring='accuracy', n_jobs=6)\n",
    "grid_result = gs.fit(X_normed, y)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 0s 551us/step - loss: 0.6532 - accuracy: 0.6491\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 0s 443us/step - loss: 0.4244 - accuracy: 0.8395\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 0s 554us/step - loss: 0.4084 - accuracy: 0.8312\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 0s 492us/step - loss: 0.3727 - accuracy: 0.8489\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.3862 - accuracy: 0.8424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23a07669820>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refit the model using the optimal parameters\n",
    "# edit: i manually set lowered the epochs to 5 to reduce overfitting.\n",
    "opti_model = create_model('Adam', 0.05, 80, 16)\n",
    "opti_model.fit(X_normed, y, batch_size=92, epochs=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_dmat = xgb.DMatrix(data=xtrain_normed,label=ytrain_val)\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(xtrain_normed, ytrain_val)\n",
    "preds = xgb_model.predict(xtest_normed)\n",
    "preds = [round(value) for value in preds]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(ytest_val, preds)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Pclass                                          Name     Sex  \\\n",
      "0          892       3                              Kelly, Mr. James    male   \n",
      "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
      "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
      "3          895       3                              Wirz, Mr. Albert    male   \n",
      "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
      "\n",
      "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
      "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
      "1  47.0      1      0   363272   7.0000   NaN        S  \n",
      "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
      "3  27.0      0      0   315154   8.6625   NaN        S  \n",
      "4  22.0      1      1  3101298  12.2875   NaN        S  \n",
      "Empty DataFrame\n",
      "Columns: [PassengerId, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked, Initial]\n",
      "Index: []\n",
      "     Pclass  Sex   Age  SibSp  Parch      Fare  title_Master  title_Miss  \\\n",
      "0         3    1  34.5      0      0    7.8292             0           0   \n",
      "1         3    0  47.0      1      0    7.0000             0           0   \n",
      "2         2    1  62.0      0      0    9.6875             0           0   \n",
      "3         3    1  27.0      0      0    8.6625             0           0   \n",
      "4         3    0  22.0      1      1   12.2875             0           0   \n",
      "..      ...  ...   ...    ...    ...       ...           ...         ...   \n",
      "413       3    1  32.1      0      0    8.0500             0           0   \n",
      "414       1    0  39.0      0      0  108.9000             0           0   \n",
      "415       3    1  38.5      0      0    7.2500             0           0   \n",
      "416       3    1  32.1      0      0    8.0500             0           0   \n",
      "417       3    1   7.4      1      1   22.3583             1           0   \n",
      "\n",
      "     title_Mr  title_Mrs  title_Other  embarked_C  embarked_Q  embarked_S  \n",
      "0           1          0            0           0           1           0  \n",
      "1           0          1            0           0           0           1  \n",
      "2           1          0            0           0           1           0  \n",
      "3           1          0            0           0           0           1  \n",
      "4           0          1            0           0           0           1  \n",
      "..        ...        ...          ...         ...         ...         ...  \n",
      "413         1          0            0           0           0           1  \n",
      "414         0          1            0           1           0           0  \n",
      "415         1          0            0           0           0           1  \n",
      "416         1          0            0           0           0           1  \n",
      "417         0          0            0           1           0           0  \n",
      "\n",
      "[418 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# ==================== apply model\n",
    "test_data = pd.read_csv(\"C:/Projects/Python/titanic/data/test.csv\")\n",
    "\n",
    "print(test_data.head())\n",
    "\n",
    "# get mean aged based on titel\n",
    "test_data['Initial']=0\n",
    "for i in data:\n",
    "    test_data['Initial']=test_data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n",
    "\n",
    "test_data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Dona','Jonkheer','Col','Rev','Capt','Sir','Don'],\n",
    "                    ['Mrs','Mrs','Miss','Mr','Mr','Mrs','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\n",
    "\n",
    "\n",
    "#print(test_data.loc[(test_data.Initial=='Other'),'Age'].mean())\n",
    "\n",
    "## Assigning the NaN Values with the Ceil values of the mean ages\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Mr'),'Age']=32.1\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Mrs'),'Age']=38.9\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Master'),'Age']=7.4\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Miss'),'Age']=21.8\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Other'),'Age']=42\n",
    "print(test_data.loc[test_data.Age.isnull()]) #So no null values left finally \n",
    "\n",
    "# retain the passengerID, used for testset\n",
    "storelist = test_data['PassengerId']\n",
    "# drop some irrelevant information\n",
    "droplist = ['PassengerId', 'Name', 'Cabin', 'Ticket']\n",
    "test_data.drop(droplist,axis=1,inplace=True)\n",
    "\n",
    "# one hot encoding of categorical values\n",
    "ohe_initial = pd.get_dummies(test_data['Initial'], prefix='title')\n",
    "ohe_embarked = pd.get_dummies(test_data['Embarked'], prefix='embarked')\n",
    "# turn sex into integers instead of string\n",
    "test_data['Sex'] = test_data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "new_df = pd.merge(test_data, ohe_initial, left_index=True, right_index=True)\n",
    "final_test_df = pd.merge(new_df, ohe_embarked, left_index=True, right_index=True)\n",
    "droplist = ['Initial', 'Embarked']\n",
    "final_test_df.drop(droplist,axis=1,inplace=True)\n",
    "print(final_test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 14)\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [nan]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "ypred 152: [nan]\n",
      "     PassengerId  Survived\n",
      "152         1044       0.0\n",
      "     PassengerId  Survived\n",
      "0            892         0\n",
      "1            893         0\n",
      "2            894         0\n",
      "3            895         0\n",
      "4            896         0\n",
      "..           ...       ...\n",
      "413         1305         0\n",
      "414         1306         1\n",
      "415         1307         0\n",
      "416         1308         0\n",
      "417         1309         1\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# data prep\n",
    "Xtest = final_test_df\n",
    "\n",
    "#print(ytest.head())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_normed = scaler.fit_transform(Xtest)\n",
    "\n",
    "print(X_normed.shape)\n",
    "\n",
    "ypred = opti_model.predict(X_normed)\n",
    "print(ypred.round())\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result['PassengerId'] = storelist\n",
    "print(\"ypred 152: \" + str(ypred[152]))\n",
    "result[\"Survived\"] = ypred.round()\n",
    "# hack, the fare is empty, but ill just predict myself\n",
    "result.iloc[152,1] = 0.0\n",
    "print(result[result['PassengerId']==1044])\n",
    "# result[result['PassengerId']==1044, 'Survived'] = 0.0\n",
    "result[\"Survived\"] = result[\"Survived\"].astype(int)\n",
    "print(result)\n",
    "result.to_csv(r'C:\\Projects\\Python\\titanic\\my_submission.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
