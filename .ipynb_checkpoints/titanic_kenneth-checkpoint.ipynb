{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\Python\\Titanic\\data\\test.csv\n",
      "C:\\Projects\\Python\\Titanic\\data\\train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # wrapper to use function from sklearn\n",
    "\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV #training and testing data split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('C:\\Projects\\Python\\Titanic\\data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "# import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "#import sns for better plots, it is handy to manage subplots\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Projects/Python/titanic/data/train.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Survived  Pclass  Sex   Age  SibSp  Parch     Fare  title_Master  \\\n",
      "0           0       3    1  22.0      1      0   7.2500             0   \n",
      "1           1       1    0  38.0      1      0  71.2833             0   \n",
      "2           1       3    0  26.0      0      0   7.9250             0   \n",
      "3           1       1    0  35.0      1      0  53.1000             0   \n",
      "4           0       3    1  35.0      0      0   8.0500             0   \n",
      "..        ...     ...  ...   ...    ...    ...      ...           ...   \n",
      "886         0       2    1  27.0      0      0  13.0000             0   \n",
      "887         1       1    0  19.0      0      0  30.0000             0   \n",
      "888         0       3    0  22.0      1      2  23.4500             0   \n",
      "889         1       1    1  26.0      0      0  30.0000             0   \n",
      "890         0       3    1  32.0      0      0   7.7500             0   \n",
      "\n",
      "     title_Miss  title_Mr  title_Mrs  title_Other  embarked_C  embarked_Q  \\\n",
      "0             0         1          0            0           0           0   \n",
      "1             0         0          1            0           1           0   \n",
      "2             1         0          0            0           0           0   \n",
      "3             0         0          1            0           0           0   \n",
      "4             0         1          0            0           0           0   \n",
      "..          ...       ...        ...          ...         ...         ...   \n",
      "886           0         0          0            1           0           0   \n",
      "887           1         0          0            0           0           0   \n",
      "888           1         0          0            0           0           0   \n",
      "889           0         1          0            0           1           0   \n",
      "890           0         1          0            0           0           1   \n",
      "\n",
      "     embarked_S  \n",
      "0             1  \n",
      "1             0  \n",
      "2             1  \n",
      "3             1  \n",
      "4             1  \n",
      "..          ...  \n",
      "886           1  \n",
      "887           1  \n",
      "888           1  \n",
      "889           0  \n",
      "890           0  \n",
      "\n",
      "[891 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# get mean aged based on titel\n",
    "data['Initial']=0\n",
    "for i in data:\n",
    "    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n",
    "\n",
    "data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n",
    "                    ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\n",
    "\n",
    "## Assigning the NaN Values with the Ceil values of the mean ages\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Mr'),'Age']=33\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Mrs'),'Age']=36\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Master'),'Age']=5\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Miss'),'Age']=22\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Other'),'Age']=46\n",
    "data.Age.isnull().any() #So no null values left finally \n",
    "\n",
    "# drop some irrelevant information\n",
    "droplist = ['PassengerId', 'Name', 'Cabin', 'Ticket']\n",
    "data.drop(droplist,axis=1,inplace=True)\n",
    "\n",
    "# one hot encoding of categorical values\n",
    "ohe_initial = pd.get_dummies(data['Initial'], prefix='title')\n",
    "ohe_embarked = pd.get_dummies(data['Embarked'], prefix='embarked')\n",
    "# turn sex into integers instead of string\n",
    "data['Sex'] = data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "new_df = pd.merge(data, ohe_initial, left_index=True, right_index=True)\n",
    "final_df = pd.merge(new_df, ohe_embarked, left_index=True, right_index=True)\n",
    "droplist = ['Initial', 'Embarked']\n",
    "final_df.drop(droplist,axis=1,inplace=True)\n",
    "\n",
    "print(final_df)\n",
    "\n",
    "#================================= end data preprocessing ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape before: (757, 14)\n",
      "xtrain shape after: (757, 14)\n",
      "(134, 14)\n",
      "(757, 14)\n",
      "(757, 1)\n",
      "(134, 14)\n",
      "(134, 1)\n",
      "[[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "#========================= start data prep\n",
    "train, test= train_test_split(final_df, test_size=0.15, stratify=final_df[\"Survived\"])\n",
    "\n",
    "Xtrain = train[train.columns[1:]]\n",
    "ytrain = train[train.columns[:1]]\n",
    "Xtest = test[test.columns[1:]]\n",
    "ytest = test[test.columns[:1]]\n",
    "\n",
    "# for cross validation we need to take the complete dataset and pass it as it takes care of test train split\n",
    "X = final_df[final_df.columns[1:]]\n",
    "y = final_df[final_df.columns[:1]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_normed = scaler.fit_transform(X)\n",
    "\n",
    "xtrain_val = Xtrain.values\n",
    "print(\"xtrain shape before: \" + str(xtrain_val.shape))\n",
    "#xtrain_val = np.reshape(xtrain_val, (-1, 757))\n",
    "print(\"xtrain shape after: \" + str(xtrain_val.shape))\n",
    "ytrain_val = ytrain.values\n",
    "#ytrain_val = np.reshape(ytrain_val, (-1, 757))\n",
    "\n",
    "xtest_val = Xtest.values\n",
    "print(xtest_val.shape)\n",
    "#xtest_val = np.reshape(xtest_val, (-1, 134))\n",
    "ytest_val = ytest.values\n",
    "#ytest_val = np.reshape(ytest_val, (-1, 134))\n",
    "\n",
    "print(xtrain_val.shape)\n",
    "print(ytrain_val.shape)\n",
    "print(xtest_val.shape)\n",
    "print(ytest_val.shape)\n",
    "\n",
    "xtrain_normed = scaler.fit_transform(xtrain_val)\n",
    "xtest_normed = scaler.fit_transform(xtest_val)\n",
    "print(ytest_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 14)\n",
      "(134, 1)\n",
      "Best: 0.834969 using {'batch_size': 90, 'epochs': 5, 'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "def create_model(optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(196, activation='relu'))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    #model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    #model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(xtest_normed.shape)\n",
    "print(ytest_val.shape)\n",
    "\n",
    "#model.fit(xtrain_normed, ytrain_val, validation_data=(xtest_normed, ytest_val), epochs=50, batch_size=4, verbose=2)\n",
    "sk_model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "#scores = cross_val_score(sk_model, X_normed, y, cv=k_fold, scoring='accuracy')\n",
    "k_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "#batch_size = [4, 8, 16, 24, 32, 64]\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "batch_size = [64, 74, 90, 128]\n",
    "epochs = [3, 5, 8, 10, 12]\n",
    "parameter_grid = dict(optimizer=optimizer, batch_size=batch_size, epochs=epochs)\n",
    "#Best: 0.834969 using {'batch_size': 90, 'epochs': 5, 'optimizer': 'Nadam'}\n",
    "\n",
    "gs = GridSearchCV(sk_model, parameter_grid, cv=k_fold, scoring='accuracy', n_jobs=6)\n",
    "grid_result = gs.fit(X_normed, y)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 0s 554us/step - loss: 0.6512 - accuracy: 0.7143\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 0s 554us/step - loss: 0.5208 - accuracy: 0.8043\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 0s 554us/step - loss: 0.4538 - accuracy: 0.8257\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 0s 665us/step - loss: 0.4197 - accuracy: 0.8346\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 0s 554us/step - loss: 0.3803 - accuracy: 0.8492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a414b5de50>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refit the model using the optimal parameters\n",
    "opti_model = create_model('Nadam')\n",
    "opti_model.fit(X_normed, y, batch_size=90, epochs=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 76.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kenneth\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\kenneth\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "xgb_dmat = xgb.DMatrix(data=xtrain_normed,label=ytrain_val)\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(xtrain_normed, ytrain_val)\n",
    "preds = xgb_model.predict(xtest_normed)\n",
    "preds = [round(value) for value in preds]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(ytest_val, preds)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Pclass                                          Name     Sex  \\\n",
      "0          892       3                              Kelly, Mr. James    male   \n",
      "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
      "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
      "3          895       3                              Wirz, Mr. Albert    male   \n",
      "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
      "\n",
      "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
      "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
      "1  47.0      1      0   363272   7.0000   NaN        S  \n",
      "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
      "3  27.0      0      0   315154   8.6625   NaN        S  \n",
      "4  22.0      1      1  3101298  12.2875   NaN        S  \n",
      "Empty DataFrame\n",
      "Columns: [PassengerId, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked, Initial]\n",
      "Index: []\n",
      "     Pclass  Sex   Age  SibSp  Parch      Fare  title_Master  title_Miss  \\\n",
      "0         3    1  34.5      0      0    7.8292             0           0   \n",
      "1         3    0  47.0      1      0    7.0000             0           0   \n",
      "2         2    1  62.0      0      0    9.6875             0           0   \n",
      "3         3    1  27.0      0      0    8.6625             0           0   \n",
      "4         3    0  22.0      1      1   12.2875             0           0   \n",
      "..      ...  ...   ...    ...    ...       ...           ...         ...   \n",
      "413       3    1  32.1      0      0    8.0500             0           0   \n",
      "414       1    0  39.0      0      0  108.9000             0           0   \n",
      "415       3    1  38.5      0      0    7.2500             0           0   \n",
      "416       3    1  32.1      0      0    8.0500             0           0   \n",
      "417       3    1   7.4      1      1   22.3583             1           0   \n",
      "\n",
      "     title_Mr  title_Mrs  title_Other  embarked_C  embarked_Q  embarked_S  \n",
      "0           1          0            0           0           1           0  \n",
      "1           0          1            0           0           0           1  \n",
      "2           1          0            0           0           1           0  \n",
      "3           1          0            0           0           0           1  \n",
      "4           0          1            0           0           0           1  \n",
      "..        ...        ...          ...         ...         ...         ...  \n",
      "413         1          0            0           0           0           1  \n",
      "414         0          1            0           1           0           0  \n",
      "415         1          0            0           0           0           1  \n",
      "416         1          0            0           0           0           1  \n",
      "417         0          0            0           1           0           0  \n",
      "\n",
      "[418 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# ==================== apply model\n",
    "test_data = pd.read_csv(\"C:/Projects/Python/titanic/data/test.csv\")\n",
    "\n",
    "print(test_data.head())\n",
    "\n",
    "# get mean aged based on titel\n",
    "test_data['Initial']=0\n",
    "for i in data:\n",
    "    test_data['Initial']=test_data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n",
    "\n",
    "test_data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Dona','Jonkheer','Col','Rev','Capt','Sir','Don'],\n",
    "                    ['Mrs','Mrs','Miss','Mr','Mr','Mrs','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\n",
    "\n",
    "\n",
    "#print(test_data.loc[(test_data.Initial=='Other'),'Age'].mean())\n",
    "\n",
    "## Assigning the NaN Values with the Ceil values of the mean ages\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Mr'),'Age']=32.1\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Mrs'),'Age']=38.9\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Master'),'Age']=7.4\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Miss'),'Age']=21.8\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Other'),'Age']=42\n",
    "print(test_data.loc[test_data.Age.isnull()]) #So no null values left finally \n",
    "\n",
    "# drop some irrelevant information\n",
    "droplist = ['PassengerId', 'Name', 'Cabin', 'Ticket']\n",
    "test_data.drop(droplist,axis=1,inplace=True)\n",
    "\n",
    "# one hot encoding of categorical values\n",
    "ohe_initial = pd.get_dummies(test_data['Initial'], prefix='title')\n",
    "ohe_embarked = pd.get_dummies(test_data['Embarked'], prefix='embarked')\n",
    "# turn sex into integers instead of string\n",
    "test_data['Sex'] = test_data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "new_df = pd.merge(test_data, ohe_initial, left_index=True, right_index=True)\n",
    "final_test_df = pd.merge(new_df, ohe_embarked, left_index=True, right_index=True)\n",
    "droplist = ['Initial', 'Embarked']\n",
    "final_test_df.drop(droplist,axis=1,inplace=True)\n",
    "print(final_test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pclass  Sex   Age  SibSp  Parch     Fare  title_Master  title_Miss  \\\n",
      "0       3    1  34.5      0      0   7.8292             0           0   \n",
      "1       3    0  47.0      1      0   7.0000             0           0   \n",
      "2       2    1  62.0      0      0   9.6875             0           0   \n",
      "3       3    1  27.0      0      0   8.6625             0           0   \n",
      "4       3    0  22.0      1      1  12.2875             0           0   \n",
      "\n",
      "   title_Mr  title_Mrs  title_Other  embarked_C  embarked_Q  embarked_S  \n",
      "0         1          0            0           0           1           0  \n",
      "1         0          1            0           0           0           1  \n",
      "2         1          0            0           0           1           0  \n",
      "3         1          0            0           0           0           1  \n",
      "4         0          1            0           0           0           1  \n",
      "[[0.08635789]\n",
      " [0.5811564 ]\n",
      " [0.08562511]\n",
      " [0.09625086]\n",
      " [0.58247757]\n",
      " [0.10993424]\n",
      " [0.7122548 ]\n",
      " [0.1682542 ]\n",
      " [0.80437887]\n",
      " [0.09278986]\n",
      " [0.08953339]\n",
      " [0.23408931]\n",
      " [0.9172297 ]\n",
      " [0.09856179]\n",
      " [0.90442526]\n",
      " [0.89024985]\n",
      " [0.14472905]\n",
      " [0.16795114]\n",
      " [0.4543434 ]\n",
      " [0.7244976 ]\n",
      " [0.31560054]\n",
      " [0.7695182 ]\n",
      " [0.907756  ]\n",
      " [0.516134  ]\n",
      " [0.9340141 ]\n",
      " [0.06147248]\n",
      " [0.93195903]\n",
      " [0.16195044]\n",
      " [0.25418234]\n",
      " [0.09165505]\n",
      " [0.12648034]\n",
      " [0.16241542]\n",
      " [0.5307945 ]\n",
      " [0.5197814 ]\n",
      " [0.4257288 ]\n",
      " [0.17747656]\n",
      " [0.5559186 ]\n",
      " [0.5566771 ]\n",
      " [0.0989044 ]\n",
      " [0.12852383]\n",
      " [0.10818592]\n",
      " [0.26935193]\n",
      " [0.07678536]\n",
      " [0.81078124]\n",
      " [0.9011341 ]\n",
      " [0.09802148]\n",
      " [0.3415072 ]\n",
      " [0.09099314]\n",
      " [0.9484292 ]\n",
      " [0.5963739 ]\n",
      " [0.31621957]\n",
      " [0.24636525]\n",
      " [0.47613376]\n",
      " [0.79181594]\n",
      " [0.21484739]\n",
      " [0.09631401]\n",
      " [0.08564487]\n",
      " [0.09786803]\n",
      " [0.09241557]\n",
      " [0.97610927]\n",
      " [0.10601071]\n",
      " [0.1518884 ]\n",
      " [0.10475549]\n",
      " [0.7312105 ]\n",
      " [0.89657295]\n",
      " [0.82137907]\n",
      " [0.7355499 ]\n",
      " [0.25025278]\n",
      " [0.4050107 ]\n",
      " [0.82110256]\n",
      " [0.7282338 ]\n",
      " [0.10164288]\n",
      " [0.55995476]\n",
      " [0.41651702]\n",
      " [0.9692498 ]\n",
      " [0.5912151 ]\n",
      " [0.08962315]\n",
      " [0.8431698 ]\n",
      " [0.15406299]\n",
      " [0.7282338 ]\n",
      " [0.74485946]\n",
      " [0.33705103]\n",
      " [0.22594085]\n",
      " [0.08953339]\n",
      " [0.15205824]\n",
      " [0.10998842]\n",
      " [0.7217616 ]\n",
      " [0.5554079 ]\n",
      " [0.73150086]\n",
      " [0.8041642 ]\n",
      " [0.61210483]\n",
      " [0.08946306]\n",
      " [0.87901706]\n",
      " [0.08962315]\n",
      " [0.43784747]\n",
      " [0.09794936]\n",
      " [0.8852091 ]\n",
      " [0.09347421]\n",
      " [0.55515975]\n",
      " [0.0884254 ]\n",
      " [0.93757653]\n",
      " [0.17279947]\n",
      " [0.09099314]\n",
      " [0.09685782]\n",
      " [0.6760239 ]\n",
      " [0.10383931]\n",
      " [0.115455  ]\n",
      " [0.09099314]\n",
      " [0.09001002]\n",
      " [0.16936466]\n",
      " [0.17091319]\n",
      " [0.7315351 ]\n",
      " [0.91772544]\n",
      " [0.73400706]\n",
      " [0.9457414 ]\n",
      " [0.1449492 ]\n",
      " [0.12436172]\n",
      " [0.48999152]\n",
      " [0.44058198]\n",
      " [0.8044735 ]\n",
      " [0.71205884]\n",
      " [0.08209059]\n",
      " [0.94439214]\n",
      " [0.09425619]\n",
      " [0.09099314]\n",
      " [0.592807  ]\n",
      " [0.10064417]\n",
      " [0.5796289 ]\n",
      " [0.13370666]\n",
      " [0.09896222]\n",
      " [0.08975425]\n",
      " [0.23350307]\n",
      " [0.41930282]\n",
      " [0.10693213]\n",
      " [0.07382429]\n",
      " [0.09885588]\n",
      " [0.14543855]\n",
      " [0.15951988]\n",
      " [0.5566542 ]\n",
      " [0.01382259]\n",
      " [0.06490594]\n",
      " [0.9207428 ]\n",
      " [0.22755885]\n",
      " [0.16603461]\n",
      " [0.2461628 ]\n",
      " [0.05387717]\n",
      " [0.29967442]\n",
      " [0.10078597]\n",
      " [0.26935193]\n",
      " [0.054923  ]\n",
      " [0.96004206]\n",
      " [0.12486988]\n",
      " [       nan]\n",
      " [0.5880923 ]\n",
      " [0.09452835]\n",
      " [0.09869099]\n",
      " [0.943329  ]\n",
      " [0.5563507 ]\n",
      " [0.24616277]\n",
      " [0.62132996]\n",
      " [0.73148125]\n",
      " [0.7407587 ]\n",
      " [0.7198655 ]\n",
      " [0.08901319]\n",
      " [0.05628887]\n",
      " [0.6019651 ]\n",
      " [0.35169482]\n",
      " [0.07621053]\n",
      " [0.94879436]\n",
      " [0.55745524]\n",
      " [0.08933225]\n",
      " [0.14328298]\n",
      " [0.10333815]\n",
      " [0.12463337]\n",
      " [0.02018276]\n",
      " [0.76016665]\n",
      " [0.74542093]\n",
      " [0.31751525]\n",
      " [0.7465596 ]\n",
      " [0.9107524 ]\n",
      " [0.15406299]\n",
      " [0.38202128]\n",
      " [0.90379775]\n",
      " [0.09099314]\n",
      " [0.9730055 ]\n",
      " [0.14017668]\n",
      " [0.7409121 ]\n",
      " [0.09078652]\n",
      " [0.01430872]\n",
      " [0.13779622]\n",
      " [0.15569338]\n",
      " [0.26863533]\n",
      " [0.6375514 ]\n",
      " [0.08821809]\n",
      " [0.86292124]\n",
      " [0.08831748]\n",
      " [0.95221794]\n",
      " [0.55495983]\n",
      " [0.16162667]\n",
      " [0.6332263 ]\n",
      " [0.7406069 ]\n",
      " [0.7816322 ]\n",
      " [0.43268934]\n",
      " [0.6314397 ]\n",
      " [0.1590957 ]\n",
      " [0.39356896]\n",
      " [0.69674826]\n",
      " [0.16035387]\n",
      " [0.9181203 ]\n",
      " [0.09800518]\n",
      " [0.09832823]\n",
      " [0.0890421 ]\n",
      " [0.23088905]\n",
      " [0.75564563]\n",
      " [0.09776288]\n",
      " [0.26677307]\n",
      " [0.7316525 ]\n",
      " [0.27698776]\n",
      " [0.9588974 ]\n",
      " [0.08962315]\n",
      " [0.8089322 ]\n",
      " [0.10172963]\n",
      " [0.80482316]\n",
      " [0.10158664]\n",
      " [0.9372194 ]\n",
      " [0.61829966]\n",
      " [0.0997096 ]\n",
      " [0.73150086]\n",
      " [0.08094838]\n",
      " [0.1452136 ]\n",
      " [0.22388977]\n",
      " [0.8295281 ]\n",
      " [0.10215622]\n",
      " [0.09102237]\n",
      " [0.4016269 ]\n",
      " [0.10257003]\n",
      " [0.28016037]\n",
      " [0.1717914 ]\n",
      " [0.6085854 ]\n",
      " [0.94807315]\n",
      " [0.9352823 ]\n",
      " [0.7739339 ]\n",
      " [0.3661341 ]\n",
      " [0.08952856]\n",
      " [0.63468283]\n",
      " [0.2736072 ]\n",
      " [0.8195878 ]\n",
      " [0.13752022]\n",
      " [0.8044735 ]\n",
      " [0.6753452 ]\n",
      " [0.64094496]\n",
      " [0.10261017]\n",
      " [0.49493134]\n",
      " [0.09965664]\n",
      " [0.08992711]\n",
      " [0.08933225]\n",
      " [0.09099314]\n",
      " [0.09472933]\n",
      " [0.714281  ]\n",
      " [0.10157493]\n",
      " [0.08462897]\n",
      " [0.10161948]\n",
      " [0.795473  ]\n",
      " [0.48248735]\n",
      " [0.22424343]\n",
      " [0.08953339]\n",
      " [0.23263136]\n",
      " [0.08933225]\n",
      " [0.5559186 ]\n",
      " [0.10639495]\n",
      " [0.3839343 ]\n",
      " [0.09099314]\n",
      " [0.964422  ]\n",
      " [0.69259965]\n",
      " [0.12463194]\n",
      " [0.6401735 ]\n",
      " [0.15507606]\n",
      " [0.14475855]\n",
      " [0.16220343]\n",
      " [0.16271636]\n",
      " [0.5576032 ]\n",
      " [0.67494816]\n",
      " [0.73150086]\n",
      " [0.6541432 ]\n",
      " [0.4933071 ]\n",
      " [0.08402604]\n",
      " [0.08915809]\n",
      " [0.33316815]\n",
      " [0.12463337]\n",
      " [0.08962315]\n",
      " [0.28671885]\n",
      " [0.71129227]\n",
      " [0.12463337]\n",
      " [0.24171248]\n",
      " [0.08497581]\n",
      " [0.09693006]\n",
      " [0.8196564 ]\n",
      " [0.09165505]\n",
      " [0.2943204 ]\n",
      " [0.09343073]\n",
      " [0.08959356]\n",
      " [0.21415487]\n",
      " [0.13347656]\n",
      " [0.09929526]\n",
      " [0.73150086]\n",
      " [0.8455806 ]\n",
      " [0.28332046]\n",
      " [0.7972854 ]\n",
      " [0.24078965]\n",
      " [0.601144  ]\n",
      " [0.10520053]\n",
      " [0.16398278]\n",
      " [0.08934674]\n",
      " [0.6892512 ]\n",
      " [0.96381795]\n",
      " [0.736546  ]\n",
      " [0.37235868]\n",
      " [0.16688335]\n",
      " [0.09576562]\n",
      " [0.1652394 ]\n",
      " [0.09685782]\n",
      " [0.15154707]\n",
      " [0.15951988]\n",
      " [0.26729396]\n",
      " [0.94302845]\n",
      " [0.099298  ]\n",
      " [0.5002184 ]\n",
      " [0.38750452]\n",
      " [0.16765994]\n",
      " [0.16601515]\n",
      " [0.77040845]\n",
      " [0.371155  ]\n",
      " [0.12463194]\n",
      " [0.6812252 ]\n",
      " [0.09579057]\n",
      " [0.274064  ]\n",
      " [0.151505  ]\n",
      " [0.09539658]\n",
      " [0.17467174]\n",
      " [0.8301504 ]\n",
      " [0.16842741]\n",
      " [0.08947936]\n",
      " [0.00527889]\n",
      " [0.9904715 ]\n",
      " [0.6528749 ]\n",
      " [0.5567882 ]\n",
      " [0.15951988]\n",
      " [0.7491827 ]\n",
      " [0.16238883]\n",
      " [0.7363574 ]\n",
      " [0.942524  ]\n",
      " [0.1590957 ]\n",
      " [0.22968584]\n",
      " [0.10772979]\n",
      " [0.46732125]\n",
      " [0.22297943]\n",
      " [0.8542567 ]\n",
      " [0.08952367]\n",
      " [0.09099314]\n",
      " [0.61247617]\n",
      " [0.00860938]\n",
      " [0.86677897]\n",
      " [0.82219535]\n",
      " [0.09625086]\n",
      " [0.95120263]\n",
      " [0.07946536]\n",
      " [0.10998994]\n",
      " [0.61048406]\n",
      " [0.9414667 ]\n",
      " [0.23237413]\n",
      " [0.17854235]\n",
      " [0.9542965 ]\n",
      " [0.196105  ]\n",
      " [0.12968889]\n",
      " [0.88856316]\n",
      " [0.97414994]\n",
      " [0.33482438]\n",
      " [0.1647963 ]\n",
      " [0.23562995]\n",
      " [0.11140311]\n",
      " [0.09099314]\n",
      " [0.10483897]\n",
      " [0.64902014]\n",
      " [0.616799  ]\n",
      " [0.15126553]\n",
      " [0.6675303 ]\n",
      " [0.09881294]\n",
      " [0.10366687]\n",
      " [0.11544272]\n",
      " [0.26626152]\n",
      " [0.37182122]\n",
      " [0.89212775]\n",
      " [0.750872  ]\n",
      " [0.12211275]\n",
      " [0.04769894]\n",
      " [0.90783906]\n",
      " [0.10888538]\n",
      " [0.9274679 ]\n",
      " [0.10063264]\n",
      " [0.09332523]\n",
      " [0.92459154]\n",
      " [0.14430124]\n",
      " [0.9313379 ]\n",
      " [0.30779904]\n",
      " [0.32744533]\n",
      " [0.28780904]\n",
      " [0.17278004]\n",
      " [0.36734402]\n",
      " [0.73146665]\n",
      " [0.47962058]\n",
      " [0.73150086]\n",
      " [0.86622405]\n",
      " [0.5593357 ]\n",
      " [0.08962315]\n",
      " [0.96748126]\n",
      " [0.08031124]\n",
      " [0.08962318]\n",
      " [0.74936736]]\n"
     ]
    }
   ],
   "source": [
    "# data prep\n",
    "Xtest = final_test_df\n",
    "\n",
    "print(Xtest.head())\n",
    "#print(ytest.head())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_normed = scaler.fit_transform(Xtest)\n",
    "\n",
    "ypred = opti_model.predict(X_normed)\n",
    "print(ypred.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
